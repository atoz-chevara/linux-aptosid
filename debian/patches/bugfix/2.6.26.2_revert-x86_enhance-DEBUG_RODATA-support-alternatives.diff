Reversion patch for e587cadd8f47e202a30712e2906a65a0606d5865, as it breaks 
virtualbox-ose:


commit e587cadd8f47e202a30712e2906a65a0606d5865
Author: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
Date:   Thu Mar 6 08:48:49 2008 -0500

    x86: enhance DEBUG_RODATA support - alternatives
    
    Fix a memcpy that should be a text_poke (in apply_alternatives).
    
    Use kernel_wp_save/kernel_wp_restore in text_poke to support DEBUG_RODATA
    correctly and so the CPU HOTPLUG special case can be removed.
    
    Add text_poke_early, for alternatives and paravirt boot-time and module load
    time patching.
    
    Changelog:
    
    - Fix text_set and text_poke alignment check (mixed up bitwise and and or)
    - Remove text_set
    - Export add_nops, so it can be used by others.
    - Document text_poke_early.
    - Remove clflush, since it breaks some VIA architectures and is not strictly
      necessary.
    - Add kerneldoc to text_poke and text_poke_early.
    - Create a second vmap instead of using the WP bit to support Xen and VMI.
    - Move local_irq disable within text_poke and text_poke_early to be able to
      be sleepable in these functions.
    
    Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
    CC: Andi Kleen <andi@firstfloor.org>
    CC: pageexec@freemail.hu
    CC: H. Peter Anvin <hpa@zytor.com>
    CC: Jeremy Fitzhardinge <jeremy@goop.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff -Nrup a/arch/x86/kernel/alternative.c b/arch/x86/kernel/alternative.c
--- a/arch/x86/kernel/alternative.c	2008-07-13 21:51:29.000000000 +0000
+++ b/arch/x86/kernel/alternative.c	2008-08-18 21:20:40.000000000 +0000
@@ -11,8 +11,6 @@
 #include <asm/mce.h>
 #include <asm/nmi.h>
 #include <asm/vsyscall.h>
-#include <asm/cacheflush.h>
-#include <asm/io.h>
 
 #define MAX_PATCH_LEN (255-1)
 
@@ -179,7 +177,7 @@ static const unsigned char*const * find_
 #endif /* CONFIG_X86_64 */
 
 /* Use this to add nops to a buffer, then text_poke the whole buffer. */
-void add_nops(void *insns, unsigned int len)
+static void add_nops(void *insns, unsigned int len)
 {
 	const unsigned char *const *noptable = find_nop_table();
 
@@ -192,7 +190,6 @@ void add_nops(void *insns, unsigned int 
 		len -= noplen;
 	}
 }
-EXPORT_SYMBOL_GPL(add_nops);
 
 extern struct alt_instr __alt_instructions[], __alt_instructions_end[];
 extern u8 *__smp_locks[], *__smp_locks_end[];
@@ -226,7 +223,7 @@ void apply_alternatives(struct alt_instr
 		memcpy(insnbuf, a->replacement, a->replacementlen);
 		add_nops(insnbuf + a->replacementlen,
 			 a->instrlen - a->replacementlen);
-		text_poke_early(instr, insnbuf, a->instrlen);
+		text_poke(instr, insnbuf, a->instrlen);
 	}
 }
 
@@ -287,6 +284,7 @@ void alternatives_smp_module_add(struct 
 				 void *text,  void *text_end)
 {
 	struct smp_alt_module *smp;
+	unsigned long flags;
 
 	if (noreplace_smp)
 		return;
@@ -312,37 +310,39 @@ void alternatives_smp_module_add(struct 
 		__func__, smp->locks, smp->locks_end,
 		smp->text, smp->text_end, smp->name);
 
-	spin_lock(&smp_alt);
+	spin_lock_irqsave(&smp_alt, flags);
 	list_add_tail(&smp->next, &smp_alt_modules);
 	if (boot_cpu_has(X86_FEATURE_UP))
 		alternatives_smp_unlock(smp->locks, smp->locks_end,
 					smp->text, smp->text_end);
-	spin_unlock(&smp_alt);
+	spin_unlock_irqrestore(&smp_alt, flags);
 }
 
 void alternatives_smp_module_del(struct module *mod)
 {
 	struct smp_alt_module *item;
+	unsigned long flags;
 
 	if (smp_alt_once || noreplace_smp)
 		return;
 
-	spin_lock(&smp_alt);
+	spin_lock_irqsave(&smp_alt, flags);
 	list_for_each_entry(item, &smp_alt_modules, next) {
 		if (mod != item->mod)
 			continue;
 		list_del(&item->next);
-		spin_unlock(&smp_alt);
+		spin_unlock_irqrestore(&smp_alt, flags);
 		DPRINTK("%s: %s\n", __func__, item->name);
 		kfree(item);
 		return;
 	}
-	spin_unlock(&smp_alt);
+	spin_unlock_irqrestore(&smp_alt, flags);
 }
 
 void alternatives_smp_switch(int smp)
 {
 	struct smp_alt_module *mod;
+	unsigned long flags;
 
 #ifdef CONFIG_LOCKDEP
 	/*
@@ -359,7 +359,7 @@ void alternatives_smp_switch(int smp)
 		return;
 	BUG_ON(!smp && (num_online_cpus() > 1));
 
-	spin_lock(&smp_alt);
+	spin_lock_irqsave(&smp_alt, flags);
 
 	/*
 	 * Avoid unnecessary switches because it forces JIT based VMs to
@@ -383,7 +383,7 @@ void alternatives_smp_switch(int smp)
 						mod->text, mod->text_end);
 	}
 	smp_mode = smp;
-	spin_unlock(&smp_alt);
+	spin_unlock_irqrestore(&smp_alt, flags);
 }
 
 #endif
@@ -411,7 +411,7 @@ void apply_paravirt(struct paravirt_patc
 
 		/* Pad the rest with nops */
 		add_nops(insnbuf + used, p->len - used);
-		text_poke_early(p->instr, insnbuf, p->len);
+		text_poke(p->instr, insnbuf, p->len);
 	}
 }
 extern struct paravirt_patch_site __start_parainstructions[],
@@ -420,6 +420,8 @@ extern struct paravirt_patch_site __star
 
 void __init alternative_instructions(void)
 {
+	unsigned long flags;
+
 	/* The patching is not fully atomic, so try to avoid local interruptions
 	   that might execute the to be patched code.
 	   Other CPUs are not running. */
@@ -428,6 +430,7 @@ void __init alternative_instructions(voi
 	stop_mce();
 #endif
 
+	local_irq_save(flags);
 	apply_alternatives(__alt_instructions, __alt_instructions_end);
 
 	/* switch to patch-once-at-boottime-only mode and free the
@@ -459,6 +462,7 @@ void __init alternative_instructions(voi
 	}
 #endif
  	apply_paravirt(__parainstructions, __parainstructions_end);
+	local_irq_restore(flags);
 
 	if (smp_alt_once)
 		free_init_pages("SMP alternatives",
@@ -471,70 +475,18 @@ void __init alternative_instructions(voi
 #endif
 }
 
-/**
- * text_poke_early - Update instructions on a live kernel at boot time
- * @addr: address to modify
- * @opcode: source of the copy
- * @len: length to copy
- *
+/*
+ * Warning:
  * When you use this code to patch more than one byte of an instruction
  * you need to make sure that other CPUs cannot execute this code in parallel.
- * Also no thread must be currently preempted in the middle of these
- * instructions. And on the local CPU you need to be protected again NMI or MCE
- * handlers seeing an inconsistent instruction while you patch.
+ * Also no thread must be currently preempted in the middle of these instructions.
+ * And on the local CPU you need to be protected again NMI or MCE handlers
+ * seeing an inconsistent instruction while you patch.
  */
-void *text_poke_early(void *addr, const void *opcode, size_t len)
+void __kprobes text_poke(void *addr, unsigned char *opcode, int len)
 {
-	unsigned long flags;
-	local_irq_save(flags);
 	memcpy(addr, opcode, len);
-	local_irq_restore(flags);
-	sync_core();
-	/* Could also do a CLFLUSH here to speed up CPU recovery; but
-	   that causes hangs on some VIA CPUs. */
-	return addr;
-}
-
-/**
- * text_poke - Update instructions on a live kernel
- * @addr: address to modify
- * @opcode: source of the copy
- * @len: length to copy
- *
- * Only atomic text poke/set should be allowed when not doing early patching.
- * It means the size must be writable atomically and the address must be aligned
- * in a way that permits an atomic write. It also makes sure we fit on a single
- * page.
- */
-void *__kprobes text_poke(void *addr, const void *opcode, size_t len)
-{
-	unsigned long flags;
-	char *vaddr;
-	int nr_pages = 2;
-	struct page *pages[2];
-	int i;
-
-	if (!core_kernel_text((unsigned long)addr)) {
-		pages[0] = vmalloc_to_page(addr);
-		pages[1] = vmalloc_to_page(addr + PAGE_SIZE);
-	} else {
-		pages[0] = virt_to_page(addr);
-		WARN_ON(!PageReserved(pages[0]));
-		pages[1] = virt_to_page(addr + PAGE_SIZE);
-	}
-	BUG_ON(!pages[0]);
-	if (!pages[1])
-		nr_pages = 1;
-	vaddr = vmap(pages, nr_pages, VM_MAP, PAGE_KERNEL);
-	BUG_ON(!vaddr);
-	local_irq_save(flags);
-	memcpy(&vaddr[(unsigned long)addr & ~PAGE_MASK], opcode, len);
-	local_irq_restore(flags);
-	vunmap(vaddr);
 	sync_core();
 	/* Could also do a CLFLUSH here to speed up CPU recovery; but
 	   that causes hangs on some VIA CPUs. */
-	for (i = 0; i < len; i++)
-		BUG_ON(((char *)addr)[i] != ((char *)opcode)[i]);
-	return addr;
 }
diff -Nrup a/include/asm-x86/alternative.h b/include/asm-x86/alternative.h
--- a/include/asm-x86/alternative.h	2008-07-13 21:51:29.000000000 +0000
+++ b/include/asm-x86/alternative.h	2008-08-18 21:13:56.000000000 +0000
@@ -155,27 +155,6 @@ static inline void apply_paravirt(struct
 #define __parainstructions_end	NULL
 #endif
 
-extern void add_nops(void *insns, unsigned int len);
-
-/*
- * Clear and restore the kernel write-protection flag on the local CPU.
- * Allows the kernel to edit read-only pages.
- * Side-effect: any interrupt handler running between save and restore will have
- * the ability to write to read-only pages.
- *
- * Warning:
- * Code patching in the UP case is safe if NMIs and MCE handlers are stopped and
- * no thread can be preempted in the instructions being modified (no iret to an
- * invalid instruction possible) or if the instructions are changed from a
- * consistent state to another consistent state atomically.
- * More care must be taken when modifying code in the SMP case because of
- * Intel's errata.
- * On the local CPU you need to be protected again NMI or MCE handlers seeing an
- * inconsistent instruction while you patch.
- * The _early version expects the memory to already be RW.
- */
-
-extern void *text_poke(void *addr, const void *opcode, size_t len);
-extern void *text_poke_early(void *addr, const void *opcode, size_t len);
+extern void text_poke(void *addr, unsigned char *opcode, int len);
 
 #endif /* _ASM_X86_ALTERNATIVE_H */
